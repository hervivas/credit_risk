categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()
numerical_features = X.select_dtypes(include=[np.number]).columns.tolist()

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=71)

# Logistic Regression Model
print("Starting Logistic Regression...")

# Preprocessing for categorical and numerical data
categorical_transformer = OneHotEncoder(handle_unknown='ignore')
numerical_transformer = StandardScaler()

# Combine preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Define the logistic regression model
logreg = LogisticRegression()

# Create a pipeline that includes preprocessing and the model
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('classifier', logreg)])

# Define the parameter distribution for RandomizedSearchCV
param_dist = {
    'classifier__penalty': ['l1', 'l2'],
    'classifier__C': stats.uniform(1, 100),
    'classifier__max_iter': [500, 700]
}

# Reduce the number of iterations to 25
random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=25, cv=5, scoring='roc_auc', n_jobs=-1)

# Fit on the entire training set
random_search.fit(X_train, y_train)

# Get the best model
best_model_logreg = random_search.best_estimator_

# Train the final model with the best hyperparameters on the entire training set
best_model_logreg.fit(X_train, y_train)

# Evaluate on the test set
evaluate_model(best_model_logreg, X_test, y_test)

# Print best hyperparameters
print("Best Hyperparameters for Logistic Regression:", random_search.best_params_)

# Random Forest Model
print("Starting Random Forest...")

# Frequency Encoding for categorical columns
class FrequencyEncoder(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.freq_map = {}

    def fit(self, X, y=None):
        for col in X.columns:
            self.freq_map[col] = X[col].value_counts(normalize=True)
        return self

    def transform(self, X):
        X_freq = X.copy()
        for col, freqs in self.freq_map.items():
            X_freq[col] = X_freq[col].map(freqs)
        return X_freq

# Select categorical columns
categorical_columns = X.select_dtypes(exclude='number').columns

# Encode categorical columns with frequency encoding
X_freq_encoded = X.copy()
freq_encoder = FrequencyEncoder()
X_freq_encoded = freq_encoder.fit_transform(X_freq_encoded[categorical_columns])

# Combine frequency-encoded categorical columns with numerical columns
X_combined = pd.concat([X_freq_encoded, X.select_dtypes(include='number')], axis=1)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.15, stratify=y, random_state=71)

# Preprocessing for numerical data
numerical_transformer = StandardScaler()

# Combine preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
    ])

# Define the random forest model
randfor = RandomForestClassifier()

# Create a pipeline that includes preprocessing and the model
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('classifier', randfor)])

# Define the search space for hyperparameters
search_space = {
    'classifier__n_estimators': (50, 200),
    'classifier__max_depth': (5, 20),
    'classifier__min_samples_split': (2, 10),
    'classifier__min_samples_leaf': (1, 5),
    'classifier__max_features': ['sqrt', 'log2'],
    'classifier__criterion': ['gini', 'entropy'],
    'classifier__class_weight': ['balanced'],
}

# Create the BayesSearchCV object
random_rf = BayesSearchCV(
    pipeline,
    search_spaces=search_space,
    n_iter=20,
    cv=3,
    scoring='roc_auc',
    n_jobs=-1,
)

# Fit the model on the entire training set
random_rf.fit(X_train, y_train)

# Get the best model with the chosen hyperparameters
best_model_rf = random_rf.best_estimator_

# Evaluate on the test set
evaluate_model(best_model_rf, X_test, y_test)

# Print the best hyperparameters found by BayesSearchCV
print("Best Hyperparameters for Random Forest:", random_rf.best_params_)

# XGBoost Model
print("Starting XGBoost...")

# Re-create the preprocessor for XGBoost since it uses OneHotEncoder
preprocessor = ColumnTransformer(
    transformers=[
        ('num', 'passthrough', numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

# Apply the transformer to both training and testing data
X_train_transformed = preprocessor.fit_transform(X_train)
X_test_transformed = preprocessor.transform(X_test)

# Create the XGBoost classifier
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', enable_categorical=True)

# Create a pipeline that includes the preprocessor and the model
model_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', xgb_model)
])

# Define parameter grid for hyperparameter tuning
param_grid = {
    'classifier__max_depth': [3, 6, 9],
    'classifier__n_estimators': [50, 100, 200],
    'classifier__learning_rate': [0.01, 0.1, 0.2],
    'classifier__subsample': [0.8, 1.0],
    'classifier__colsample_bytree': [0.8, 1.0],
    'classifier__scale_pos_weight': [(len(y_train) - sum(y_train)) / sum(y_train)]
}

# Create the StratifiedKFold object with 5 folds
stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=71)

# Create the GridSearchCV object with the pipeline
grid_xgb = GridSearchCV(estimator=model_pipeline, param_grid=param_grid, scoring='roc_auc', cv=stratified_cv, verbose=1, n_jobs=-1)

# Fit the GridSearchCV on the training data
grid_xgb.fit(X_train, y_train)

# Get the best model with the chosen hyperparameters
best_model_xgb = grid_xgb.best_estimator_

# Evaluate the final model on the unseen testing set
evaluate_model(best_model_xgb, X_test, y_test)

# Print the best hyperparameters found by GridSearchCV
print("Best Hyperparameters for XGBoost:", grid_xgb.best_params_)
